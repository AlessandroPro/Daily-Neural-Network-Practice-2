{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4096)\n"
     ]
    }
   ],
   "source": [
    "data = fetch_olivetti_faces()\n",
    "face_images = data.data\n",
    "print(face_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0,
     20,
     29,
     62,
     87,
     98,
     104,
     107,
     312
    ]
   },
   "outputs": [],
   "source": [
    "def _gs_decorrelation(w, W, j):\n",
    "    \"\"\"\n",
    "    Orthonormalize w wrt the first j rows of W\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray of shape(n)\n",
    "        Array to be orthogonalized\n",
    "    W : ndarray of shape(p, n)\n",
    "        Null space definition\n",
    "    j : int < p\n",
    "        The no of (from the first) rows of Null space W wrt which w is\n",
    "        orthogonalized.\n",
    "    Notes\n",
    "    -----\n",
    "    Assumes that W is orthogonal\n",
    "    w changed in place\n",
    "    \"\"\"\n",
    "    w -= np.dot(np.dot(w, W[:j].T), W[:j])\n",
    "    return w\n",
    "\n",
    "def _sym_decorrelation(W):\n",
    "    \"\"\" Symmetric decorrelation\n",
    "    i.e. W <- (W * W.T) ^{-1/2} * W\n",
    "    \"\"\"\n",
    "    s, u = linalg.eigh(np.dot(W, W.T))\n",
    "    # u (resp. s) contains the eigenvectors (resp. square roots of\n",
    "    # the eigenvalues) of W * W.T\n",
    "    return np.dot(np.dot(u * (1. / np.sqrt(s)), u.T), W)\n",
    "\n",
    "def _ica_def(X, tol, g, fun_args, max_iter, w_init):\n",
    "    \"\"\"Deflationary FastICA using fun approx to neg-entropy function\n",
    "    Used internally by FastICA.\n",
    "    \"\"\"\n",
    "\n",
    "    n_components = w_init.shape[0]\n",
    "    W = np.zeros((n_components, n_components), dtype=X.dtype)\n",
    "    n_iter = []\n",
    "\n",
    "    # j is the index of the extracted component\n",
    "    for j in range(n_components):\n",
    "        w = w_init[j, :].copy()\n",
    "        w /= np.sqrt((w ** 2).sum())\n",
    "\n",
    "        for i in moves.xrange(max_iter):\n",
    "            gwtx, g_wtx = g(np.dot(w.T, X), fun_args)\n",
    "\n",
    "            w1 = (X * gwtx).mean(axis=1) - g_wtx.mean() * w\n",
    "\n",
    "            _gs_decorrelation(w1, W, j)\n",
    "\n",
    "            w1 /= np.sqrt((w1 ** 2).sum())\n",
    "\n",
    "            lim = np.abs(np.abs((w1 * w).sum()) - 1)\n",
    "            w = w1\n",
    "            if lim < tol:\n",
    "                break\n",
    "\n",
    "        n_iter.append(i + 1)\n",
    "        W[j, :] = w\n",
    "\n",
    "    return W, max(n_iter)\n",
    "\n",
    "def _ica_par(X, tol, g, fun_args, max_iter, w_init):\n",
    "    \"\"\"Parallel FastICA.\n",
    "    Used internally by FastICA --main loop\n",
    "    \"\"\"\n",
    "    W = _sym_decorrelation(w_init)\n",
    "    del w_init\n",
    "    p_ = float(X.shape[1])\n",
    "    for ii in moves.xrange(max_iter):\n",
    "        gwtx, g_wtx = g(np.dot(W, X), fun_args)\n",
    "        W1 = _sym_decorrelation(np.dot(gwtx, X.T) / p_\n",
    "                                - g_wtx[:, np.newaxis] * W)\n",
    "        del gwtx, g_wtx\n",
    "        # builtin max, abs are faster than numpy counter parts.\n",
    "        lim = max(abs(abs(np.diag(np.dot(W1, W.T))) - 1))\n",
    "        W = W1\n",
    "        if lim < tol:\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn('FastICA did not converge. Consider increasing '\n",
    "                      'tolerance or the maximum number of iterations.')\n",
    "\n",
    "    return W, ii + 1\n",
    "\n",
    "# Some standard non-linear functions.\n",
    "# XXX: these should be optimized, as they can be a bottleneck.\n",
    "def _logcosh(x, fun_args=None):\n",
    "    alpha = fun_args.get('alpha', 1.0)  # comment it out?\n",
    "\n",
    "    x *= alpha\n",
    "    gx = np.tanh(x, x)  # apply the tanh inplace\n",
    "    g_x = np.empty(x.shape[0])\n",
    "    # XXX compute in chunks to avoid extra allocation\n",
    "    for i, gx_i in enumerate(gx):  # please don't vectorize.\n",
    "        g_x[i] = (alpha * (1 - gx_i ** 2)).mean()\n",
    "    return gx, g_x\n",
    "\n",
    "def _exp(x, fun_args):\n",
    "    exp = np.exp(-(x ** 2) / 2)\n",
    "    gx = x * exp\n",
    "    g_x = (1 - x ** 2) * exp\n",
    "    return gx, g_x.mean(axis=-1)\n",
    "\n",
    "def _cube(x, fun_args):\n",
    "    return x ** 3, (3 * x ** 2).mean(axis=-1)\n",
    "\n",
    "def fastica(X, n_components=None, algorithm=\"parallel\", whiten=True,\n",
    "            fun=\"logcosh\", fun_args=None, max_iter=200, tol=1e-04, w_init=None,\n",
    "            random_state=None, return_X_mean=False, compute_sources=True,\n",
    "            return_n_iter=False):\n",
    "    \"\"\"Perform Fast Independent Component Analysis.\n",
    "    Read more in the :ref:`User Guide <ICA>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    n_components : int, optional\n",
    "        Number of components to extract. If None no dimension reduction\n",
    "        is performed.\n",
    "    algorithm : {'parallel', 'deflation'}, optional\n",
    "        Apply a parallel or deflational FASTICA algorithm.\n",
    "    whiten : boolean, optional\n",
    "        If True perform an initial whitening of the data.\n",
    "        If False, the data is assumed to have already been\n",
    "        preprocessed: it should be centered, normed and white.\n",
    "        Otherwise you will get incorrect results.\n",
    "        In this case the parameter n_components will be ignored.\n",
    "    fun : string or function, optional. Default: 'logcosh'\n",
    "        The functional form of the G function used in the\n",
    "        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n",
    "        or 'cube'.\n",
    "        You can also provide your own function. It should return a tuple\n",
    "        containing the value of the function, and of its derivative, in the\n",
    "        point. Example:\n",
    "        def my_g(x):\n",
    "            return x ** 3, 3 * x ** 2\n",
    "    fun_args : dictionary, optional\n",
    "        Arguments to send to the functional form.\n",
    "        If empty or None and if fun='logcosh', fun_args will take value\n",
    "        {'alpha' : 1.0}\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations to perform.\n",
    "    tol : float, optional\n",
    "        A positive scalar giving the tolerance at which the\n",
    "        un-mixing matrix is considered to have converged.\n",
    "    w_init : (n_components, n_components) array, optional\n",
    "        Initial un-mixing array of dimension (n.comp,n.comp).\n",
    "        If None (default) then an array of normal r.v.'s is used.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    return_X_mean : bool, optional\n",
    "        If True, X_mean is returned too.\n",
    "    compute_sources : bool, optional\n",
    "        If False, sources are not computed, but only the rotation matrix.\n",
    "        This can save memory when working with big data. Defaults to True.\n",
    "    return_n_iter : bool, optional\n",
    "        Whether or not to return the number of iterations.\n",
    "    Returns\n",
    "    -------\n",
    "    K : array, shape (n_components, n_features) | None.\n",
    "        If whiten is 'True', K is the pre-whitening matrix that projects data\n",
    "        onto the first n_components principal components. If whiten is 'False',\n",
    "        K is 'None'.\n",
    "    W : array, shape (n_components, n_components)\n",
    "        Estimated un-mixing matrix.\n",
    "        The mixing matrix can be obtained by::\n",
    "            w = np.dot(W, K.T)\n",
    "            A = w.T * (w * w.T).I\n",
    "    S : array, shape (n_samples, n_components) | None\n",
    "        Estimated source matrix\n",
    "    X_mean : array, shape (n_features, )\n",
    "        The mean over features. Returned only if return_X_mean is True.\n",
    "    n_iter : int\n",
    "        If the algorithm is \"deflation\", n_iter is the\n",
    "        maximum number of iterations run across all components. Else\n",
    "        they are just the number of iterations taken to converge. This is\n",
    "        returned only when return_n_iter is set to `True`.\n",
    "    Notes\n",
    "    -----\n",
    "    The data matrix X is considered to be a linear combination of\n",
    "    non-Gaussian (independent) components i.e. X = AS where columns of S\n",
    "    contain the independent components and A is a linear mixing\n",
    "    matrix. In short ICA attempts to `un-mix' the data by estimating an\n",
    "    un-mixing matrix W where ``S = W K X.``\n",
    "    This implementation was originally made for data of shape\n",
    "    [n_features, n_samples]. Now the input is transposed\n",
    "    before the algorithm is applied. This makes it slightly\n",
    "    faster for Fortran-ordered input.\n",
    "    Implemented using FastICA:\n",
    "    `A. Hyvarinen and E. Oja, Independent Component Analysis:\n",
    "    Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n",
    "    pp. 411-430`\n",
    "    \"\"\"\n",
    "#     random_state = check_random_state(random_state)\n",
    "    fun_args = {} if fun_args is None else fun_args\n",
    "    # make interface compatible with other decompositions\n",
    "    # a copy is required only for non whitened data\n",
    "#     X = check_array(X, copy=whiten, dtype=FLOAT_DTYPES).T\n",
    "\n",
    "    alpha = fun_args.get('alpha', 1.0)\n",
    "    if not 1 <= alpha <= 2:\n",
    "        raise ValueError('alpha must be in [1,2]')\n",
    "\n",
    "    if fun == 'logcosh':\n",
    "        g = _logcosh\n",
    "    elif fun == 'exp':\n",
    "        g = _exp\n",
    "    elif fun == 'cube':\n",
    "        g = _cube\n",
    "    elif callable(fun):\n",
    "        def g(x, fun_args):\n",
    "            return fun(x, **fun_args)\n",
    "    else:\n",
    "        exc = ValueError if isinstance(fun, six.string_types) else TypeError\n",
    "        raise exc(\"Unknown function %r;\"\n",
    "                  \" should be one of 'logcosh', 'exp', 'cube' or callable\"\n",
    "                  % fun)\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    if not whiten and n_components is not None:\n",
    "        n_components = None\n",
    "        warnings.warn('Ignoring n_components with whiten=False.')\n",
    "\n",
    "    if n_components is None:\n",
    "        n_components = min(n, p)\n",
    "    if (n_components > min(n, p)):\n",
    "        n_components = min(n, p)\n",
    "        warnings.warn('n_components is too large: it will be set to %s' % n_components)\n",
    "\n",
    "    if whiten:\n",
    "        # Centering the columns (ie the variables)\n",
    "        X_mean = X.mean(axis=-1)\n",
    "        X -= X_mean[:, np.newaxis]\n",
    "\n",
    "        # Whitening and preprocessing by PCA\n",
    "        u, d, _ = linalg.svd(X, full_matrices=False)\n",
    "\n",
    "        del _\n",
    "        K = (u / d).T[:n_components]  # see (6.33) p.140\n",
    "        del u, d\n",
    "        X1 = np.dot(K, X)\n",
    "        # see (13.6) p.267 Here X1 is white and data\n",
    "        # in X has been projected onto a subspace by PCA\n",
    "        X1 *= np.sqrt(p)\n",
    "    else:\n",
    "        # X must be casted to floats to avoid typing issues with numpy\n",
    "        # 2.0 and the line below\n",
    "        X1 = as_float_array(X, copy=False)  # copy has been taken care of\n",
    "\n",
    "    if w_init is None:\n",
    "        w_init = np.asarray(random_state.normal(size=(n_components,\n",
    "                            n_components)), dtype=X1.dtype)\n",
    "\n",
    "    else:\n",
    "        w_init = np.asarray(w_init)\n",
    "        if w_init.shape != (n_components, n_components):\n",
    "            raise ValueError('w_init has invalid shape -- should be %(shape)s'\n",
    "                             % {'shape': (n_components, n_components)})\n",
    "\n",
    "    kwargs = {'tol': tol,\n",
    "              'g': g,\n",
    "              'fun_args': fun_args,\n",
    "              'max_iter': max_iter,\n",
    "              'w_init': w_init}\n",
    "\n",
    "    if algorithm == 'parallel':\n",
    "        W, n_iter = _ica_par(X1, **kwargs)\n",
    "    elif algorithm == 'deflation':\n",
    "        W, n_iter = _ica_def(X1, **kwargs)\n",
    "    else:\n",
    "        raise ValueError('Invalid algorithm: must be either `parallel` or'\n",
    "                         ' `deflation`.')\n",
    "    del X1\n",
    "\n",
    "    if whiten:\n",
    "        if compute_sources:\n",
    "            S = np.dot(np.dot(W, K), X).T\n",
    "        else:\n",
    "            S = None\n",
    "        if return_X_mean:\n",
    "            if return_n_iter:\n",
    "                return K, W, S, X_mean, n_iter\n",
    "            else:\n",
    "                return K, W, S, X_mean\n",
    "        else:\n",
    "            if return_n_iter:\n",
    "                return K, W, S, n_iter\n",
    "            else:\n",
    "                return K, W, S\n",
    "\n",
    "    else:\n",
    "        if compute_sources:\n",
    "            S = np.dot(W, X).T\n",
    "        else:\n",
    "            S = None\n",
    "        if return_X_mean:\n",
    "            if return_n_iter:\n",
    "                return None, W, S, None, n_iter\n",
    "            else:\n",
    "                return None, W, S, None\n",
    "        else:\n",
    "            if return_n_iter:\n",
    "                return None, W, S, n_iter\n",
    "            else:\n",
    "                return None, W, S\n",
    "\n",
    "class FastICA():\n",
    "    \"\"\"FastICA: a fast algorithm for Independent Component Analysis.\n",
    "    Read more in the :ref:`User Guide <ICA>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, optional\n",
    "        Number of components to use. If none is passed, all are used.\n",
    "    algorithm : {'parallel', 'deflation'}\n",
    "        Apply parallel or deflational algorithm for FastICA.\n",
    "    whiten : boolean, optional\n",
    "        If whiten is false, the data is already considered to be\n",
    "        whitened, and no whitening is performed.\n",
    "    fun : string or function, optional. Default: 'logcosh'\n",
    "        The functional form of the G function used in the\n",
    "        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n",
    "        or 'cube'.\n",
    "        You can also provide your own function. It should return a tuple\n",
    "        containing the value of the function, and of its derivative, in the\n",
    "        point. Example:\n",
    "        def my_g(x):\n",
    "            return x ** 3, 3 * x ** 2\n",
    "    fun_args : dictionary, optional\n",
    "        Arguments to send to the functional form.\n",
    "        If empty and if fun='logcosh', fun_args will take value\n",
    "        {'alpha' : 1.0}.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations during fit.\n",
    "    tol : float, optional\n",
    "        Tolerance on update at each iteration.\n",
    "    w_init : None of an (n_components, n_components) ndarray\n",
    "        The mixing matrix to be used to initialize the algorithm.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    Attributes\n",
    "    ----------\n",
    "    components_ : 2D array, shape (n_components, n_features)\n",
    "        The unmixing matrix.\n",
    "    mixing_ : array, shape (n_features, n_components)\n",
    "        The mixing matrix.\n",
    "    n_iter_ : int\n",
    "        If the algorithm is \"deflation\", n_iter is the\n",
    "        maximum number of iterations run across all components. Else\n",
    "        they are just the number of iterations taken to converge.\n",
    "    Notes\n",
    "    -----\n",
    "    Implementation based on\n",
    "    `A. Hyvarinen and E. Oja, Independent Component Analysis:\n",
    "    Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n",
    "    pp. 411-430`\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=None, algorithm='parallel', whiten=True,\n",
    "                 fun='logcosh', fun_args=None, max_iter=200, tol=1e-4,\n",
    "                 w_init=None, random_state=None):\n",
    "        super(FastICA, self).__init__()\n",
    "        self.n_components = n_components\n",
    "        self.algorithm = algorithm\n",
    "        self.whiten = whiten\n",
    "        self.fun = fun\n",
    "        self.fun_args = fun_args\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.w_init = w_init\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _fit(self, X, compute_sources=False):\n",
    "        \"\"\"Fit the model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        compute_sources : bool\n",
    "            If False, sources are not computes but only the rotation matrix.\n",
    "            This can save memory when working with big data. Defaults to False.\n",
    "        Returns\n",
    "        -------\n",
    "            X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        fun_args = {} if self.fun_args is None else self.fun_args\n",
    "        whitening, unmixing, sources, X_mean, self.n_iter_ = fastica(\n",
    "            X=X, n_components=self.n_components, algorithm=self.algorithm,\n",
    "            whiten=self.whiten, fun=self.fun, fun_args=fun_args,\n",
    "            max_iter=self.max_iter, tol=self.tol, w_init=self.w_init,\n",
    "            random_state=self.random_state, return_X_mean=True,\n",
    "            compute_sources=compute_sources, return_n_iter=True)\n",
    "\n",
    "        if self.whiten:\n",
    "            self.components_ = np.dot(unmixing, whitening)\n",
    "            self.mean_ = X_mean\n",
    "            self.whitening_ = whitening\n",
    "        else:\n",
    "            self.components_ = unmixing\n",
    "\n",
    "        self.mixing_ = linalg.pinv(self.components_)\n",
    "\n",
    "        if compute_sources:\n",
    "            self.__sources = sources\n",
    "\n",
    "        return sources\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit the model and recover the sources from X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : Ignored.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        return self._fit(X, compute_sources=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : Ignored.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self._fit(X, compute_sources=False)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y='deprecated', copy=True):\n",
    "        \"\"\"Recover the sources from X (apply the unmixing matrix).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data to transform, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : (ignored)\n",
    "            .. deprecated:: 0.19\n",
    "               This parameter will be removed in 0.21.\n",
    "        copy : bool (optional)\n",
    "            If False, data passed to fit are overwritten. Defaults to True.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        if not isinstance(y, string_types) or y != 'deprecated':\n",
    "            warnings.warn(\"The parameter y on transform() is \"\n",
    "                          \"deprecated since 0.19 and will be removed in 0.21\",DeprecationWarning)\n",
    "\n",
    "        check_is_fitted(self, 'mixing_')\n",
    "\n",
    "        X = check_array(X, copy=copy, dtype=FLOAT_DTYPES)\n",
    "        if self.whiten:\n",
    "            X -= self.mean_\n",
    "\n",
    "        return np.dot(X, self.components_.T)\n",
    "\n",
    "    def inverse_transform(self, X, copy=True):\n",
    "        \"\"\"Transform the sources back to the mixed data (apply mixing matrix).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_components)\n",
    "            Sources, where n_samples is the number of samples\n",
    "            and n_components is the number of components.\n",
    "        copy : bool (optional)\n",
    "            If False, data passed to fit are overwritten. Defaults to True.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'mixing_')\n",
    "\n",
    "        X = check_array(X, copy=(copy and self.whiten), dtype=FLOAT_DTYPES)\n",
    "        X = np.dot(X, self.mixing_.T)\n",
    "        if self.whiten:\n",
    "            X += self.mean_\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = FastICA(n_components=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b936cc6db112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-8abb1f7af33b>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \"\"\"\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-8abb1f7af33b>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, compute_sources)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_X_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             compute_sources=compute_sources, return_n_iter=True)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-8abb1f7af33b>\u001b[0m in \u001b[0;36mfastica\u001b[0;34m(X, n_components, algorithm, whiten, fun, fun_args, max_iter, tol, w_init, random_state, return_X_mean, compute_sources, return_n_iter)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         w_init = np.asarray(random_state.normal(size=(n_components,\n\u001b[0m\u001b[1;32m    258\u001b[0m                             n_components)), dtype=X1.dtype)\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'normal'"
     ]
    }
   ],
   "source": [
    "temp = algo.fit_transform(face_images.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
